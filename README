# CSE 132 Final Project

Students will build an automated pipeline that generates offensive security commands aligned with the MITRE ATT&CK Framework. This project focuses on understanding adversarial techniques and exploring AI-assisted security testing tools.

## Objectives
*   Develop a multi-agent pipeline for command generation
*   Leverage LLMs to automatically generate commands for specific ATT&CK techniques
*   Validate command correctness and relevance to specified techniques


## Resources
Here are some resources for inspiration, but you can use whatever you want to complete this project. You can finetune a model, generate commands with a current model, generate command line data from software code datasets, use RAG, generate pipelines with all of this.

### **Modal** (Our sponsor for $500 credits for every student!)
We are running these notebooks on Modal, and so you'll need to read the [Modal Documentation](https://modal.com/docs) for help. They have guides, examples, and references that will answer a lot of your questions. (Remember when you're writing code in your notebook, treat it like local code and don't add any .remote() calls). If you haven't receive an e-mail with access for credits, let us know!

### **Inference:** This is useful when you call a model (LLM)

Models are very verbose, if you want to force structured output from models, you can use: https://github.com/dottxt-ai/outlines

RAG is a technology where you can get relevant information from a big file or database to embed it into your prompt. If you have a huge document, you may not need it all, but just parts of it. With RAG, you'll get what you need from your document or dataset. Here's an example on how to use it:
https://modal.com/notebooks/sevalder/_/nb-Gi6vnvDbDwPgAzqw6tdyPO

**Ideas for RAG:**

*   Get the MITRE Attack definitions from a document
*   Get examples from a huge dataset
*   Search over man pages or command descriptions


### **Training:** This is useful if you want to make your model better at a certain task

This is useful if you want to train a model by yourself. We built a functional Jupyter Notebook that trains a model using a specific dataset.
https://modal.com/notebooks/najilau/_/nb-d1hbAytdoaUoDYSo47cLx5

This is just an example, try to understand the code and use what you think is appropiate

**Bash Command Dataset**:  
https://huggingface.co/datasets/aelhalili/bash-commands-dataset

### Models

We recommend choosing an open source model like Qwen, Llama 3.1, etc on HuggingFace. As you have $500 in credits (you may choose the GPU being used), we recommend you choose a model between 7B to 32B.

### Pipeline

Think of it as a series of modular stages, each with a clear responsibility. Instead of one giant “black box” prompt, you chain together smaller, specialized steps. This makes the system:

* 	Reproducible → you can rerun the same steps and get consistent results.
* 	Transparent → each stage has a clear role (e.g., retrieval, generation, validation).
* 	Composable → you can swap in different tools (LangChain, DSPy, Outlines, etc.) without breaking the whole system.

An example would be:

For generating adversarial commands aligned with MITRE ATT&CK, a pipeline might look like this:
1. 	Input / Task Definition
 	Student specifies a technique ID (e.g., ) or description (“Command and Scripting Interpreter”).
2. 	Retrieval (optional, RAG)
 	Pull relevant definitions, examples, or man page snippets from a knowledge base.

 	Example: use LangChain + Chroma to fetch ATT&CK descriptions.
3. 	Generation (LLM Inference)
 	Call a local or hosted LLM to propose candidate commands.

 	Example: DSPy  module or Hugging Face pipeline.
4. 	Structuring / Constraining
 	Force the model to output in a predictable schema (JSON, Pydantic, regex).

 	Tools: Outlines, Guardrails, Instructor.
5. 	Validation / Filtering
 	Check if the command is syntactically valid, relevant to the technique, or passes a regex check.

 	Example: ensure wmic appears in T1047 outputs.
6. 	Output / Logging
 	Store results in a dataset for grading, benchmarking, or further analysis

## Final artifacts we expect to see:  
* **A generated dataset** (from 500 to 700 samples, any other sample would be omited) pushed to HuggingFace as a public dataset (as a parquet file). The name of the column containing the command **MUST** be called Command.
* **Project Report (PDF) of at least 4 pages long discussing:**
  + Links to your modal notebook (set it to public access)
	+ Link to your public HuggingFace dataset
  + Description of your approach (such as steps, model choice, hyperparameters, training duration)  
	+ Your pipeline (how did you create and validate the commands),
	+ Design considerations,
	+ Challenges encountered and how you addressed them,
	+ Insights of the results (do the results make sense, what was surprising, which results took you back to the drawing board etc.) choose 5-10 example generated commands with brief quality commentary
	+ Lessons learned.

## Grading criteria:

	+ Syntactic correctness (would the command run in a real system)? 20%
		+ We will run a pipeline to check this!
	+ Semantic correctness (are the MITRE labels appropriate)? 25%
		+ We will run a pipeline to check this!
	+ Command diversity (we will evaluate the similarity of your commands to publicly available datasets as well as previously submitted class datasets, if it is too similar to other projects or to public examples, the score will be lower) 25%
		+ We will run a command similarity over all public datasets!
	+ Quality of the model notebook 20%
	+ Quality of the PDF report 10%


## Deadlines


*   November 26 (Optional): If you submit the dataset by this date, you will get a preliminary grade of the dataset by December 2.
*   December 12: Final submission (all required artifacts).
